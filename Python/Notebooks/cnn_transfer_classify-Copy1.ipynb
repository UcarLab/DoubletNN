{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "#import keras as kr\n",
    "import sklearn as skl\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten, BatchNormalization, Conv1D, MaxPooling1D, Activation, UpSampling1D,Reshape\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import feather\n",
    "import keras_metrics\n",
    "## Import up sound alert dependencies\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "## Insert whatever audio file you want above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_validate=pd.read_pickle(\"CZI.PBMC.y.validate.nn.pkl\")\n",
    "#y_test=pd.read_pickle(\"CZI.PBMC.y.test.nn.pkl\")\n",
    "#X=pd.read_pickle(\"CZI.PBMC.full.X.nn.sorted.cleaned.pkl\")\n",
    "#y_train=pd.read_pickle(\"CZI.PBMC.y.tra\")\n",
    "y_train=pd.read_feather('CZI.PBMC.y.train.nn.feather').values\n",
    "X_train=pd.read_feather('CZI.PBMC.X.train.nn.feather').values\n",
    "#X_train=pd.read_feather('CZI.PBMC.X.train.nn.notnorm.feather')\n",
    "\n",
    "\n",
    "y_validate=pd.read_feather('CZI.PBMC.y.validate.nn.feather').values\n",
    "X_validate=pd.read_feather('CZI.PBMC.X.validate.nn.feather').values\n",
    "#X_validate=pd.read_feather('CZI.PBMC.X.validate.nn.notnorm.feather')\n",
    "\n",
    "\n",
    "N_train_dat=X_train.shape[0]\n",
    "#N_test_dat=\n",
    "N_val_dat=X_validate.shape[0]\n",
    "N_feat=X_validate.shape[1]\n",
    "\n",
    "#X_train=X_train.values.reshape(N_train_dat,N_feat,1)\n",
    "#X_validate=X_validate.values.reshape(N_val_dat,N_feat,1)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_validate_minmax = min_max_scaler.fit_transform(X_validate)\n",
    "\n",
    "X_train_minmax=X_train_minmax.reshape(N_train_dat,N_feat,1)\n",
    "X_validate_minmax=X_validate_minmax.reshape(N_val_dat,N_feat,1)\n",
    "\n",
    "\n",
    "#input_dir = os.getcwd()\n",
    "#genes=pd.read_csv(\"genes.nn.csv\").x[1:]\n",
    "\n",
    "#counts_matrix={}\n",
    "#for i in range(1,11,2):\n",
    "#    brcds_1=pd.read_csv(input_dir+'/CZI.PBMC.'+str(i)+'.brcds').x\n",
    "#    counts_matrix_1=scipy.io.mmread(input_dir + '/CZI.PBMC.'+str(i)+'.umis.mtx').T.tocsc()\n",
    "#    brcds_2=pd.read_csv(input_dir+'/CZI.PBMC.'+str(i+1)+'.brcds').x\n",
    "#    counts_matrix_2=scipy.io.mmread(input_dir + '/CZI.PBMC.'+str(i+1)+'.umis.mtx').T.tocsc()\n",
    "#    df_1=pd.DataFrame(counts_matrix_1.todense(),index=brcds_1,columns=genes)\n",
    " #   df_2=pd.DataFrame(counts_matrix_2.todense(),index=brcds_2,columns=genes)\n",
    "#    if i==1:\n",
    "#        czi_pbmc=pd.concat([df_1,df_2])\n",
    "#    else:\n",
    "#        czi_pbmc=pd.concat([czi_pbmc,df_1,df_2])\n",
    "#        \n",
    "#        \n",
    "#czi_pbmc.to_pickle(\"CZI.PBMC.full.nn.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_x_train = pd.read_csv(\"/projects/ucar-lab/danaco/bncmrk-dblts/DF/input/CZI.PBMC/CZI.PBMC.2.train.csv\", header=None)\n",
    "#df_x_validate = pd.read_csv(\"/projects/ucar-lab/danaco/bncmrk-dblts/DF/input/CZI.PBMC/CZI.PBMC.2.validate.csv\", header=None)\n",
    "#df_x_test = pd.read_csv(\"/projects/ucar-lab/danaco/bncmrk-dblts/DF/input/CZI.PBMC/CZI.PBMC.2.test.csv\", header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_targ = self.model.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\" — vali_f1: %f — vali_precision: %f — vali_recall %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    " \n",
    "\n",
    "metricS = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dnn_autoencoder_clr.h5',\n",
       " 'CZI.PBMC.X.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.csv',\n",
       " 'CZI.PBMC.new.validate.barcodes.pkl',\n",
       " 'cnn_autoencoder_plain_moreepoch.h5',\n",
       " 'iris.csv',\n",
       " 'CZI.PBMC.5.brcds',\n",
       " '.ipynb_checkpoints',\n",
       " 'CZI.PBMC.8.brcds',\n",
       " 'CZI.PBMC.test.barcodes.pkl',\n",
       " 'CZI.PBMC.2.umis.mtx',\n",
       " 'CZI.PBMC.3.brcds',\n",
       " 'CZI.PBMC.3.test-Copy1.csv',\n",
       " 'CZI.PBMC.X.validate.nn.csv',\n",
       " 'CZI.PBMC.9.umis.mtx',\n",
       " 'CZI.PBMC.X.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596475',\n",
       " 'data_line_save3.py',\n",
       " 'CZI.PBMC.full.X.nn.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.notnorm.feather',\n",
       " 'cnn_autoencoder_lasso.h5',\n",
       " 'CZI.PBMC.X.test.nn.feather',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.feather',\n",
       " 'CZI.PBMC.y.train.nn.feather',\n",
       " 'CZI.PBMC.train.barcodes.pkl',\n",
       " 'CZI.PBMC.new.test.barcodes.pkl',\n",
       " 'czi_pbmc_notnorm.mtx',\n",
       " 'data_prep_pbs.sh',\n",
       " 'first_try.ipynb',\n",
       " 'data_line_save_pbs2.sh',\n",
       " 'CZI.PBMC.X.test.nn.pkl',\n",
       " 'sonar_tutorial.ipynb',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.feather',\n",
       " 'data_line_save_feather.e10596475',\n",
       " 'CZI.PBMC.6.brcds',\n",
       " 'data_line_save_pbs4.sh',\n",
       " 'CZI.PBMC.non.neg.brcds.GMM.csv',\n",
       " 'doublet_nn_data_line_save.e10554779',\n",
       " 'data_line_save_feather.e10558828',\n",
       " 'CZI.PBMC.8.umis.mtx',\n",
       " 'data_line_save_feather.e10596482',\n",
       " 'data_line_save_feather.o10558828',\n",
       " 'doublet_nn_data_prep.o10538694',\n",
       " 'doublet_nn_data_line_save.e10551142',\n",
       " 'data_line_notnorm.py',\n",
       " 'doublet_nn_data_prep.e10545808',\n",
       " 'iris_tutorial.ipynb',\n",
       " 'cnn_transfer_classify.ipynb',\n",
       " 'CZI.PBMC.full.y.nn.sorted.cleaned.feather',\n",
       " 'CZI.PBMC.y.train.nn2.pkl',\n",
       " 'cnn_autoencoder.ipynb',\n",
       " 'dnn_autoencoder.ipynb',\n",
       " 'CZI.PBMC.validate.barcodes.pkl',\n",
       " 'dede.pkl',\n",
       " 'doublet_nn_data_line_save.e10554998',\n",
       " 'CZI.PBMC.new.test.locs.pkl',\n",
       " 'doublet_nn_data_line_save.o10551142',\n",
       " 'CZI.PBMC.y.validate.nn.feather',\n",
       " 'data_line_save2.py',\n",
       " 'sonar.csv',\n",
       " 'CZI.PBMC.y.test.nn.feather',\n",
       " 'CZI.PBMC.gmm.sorted.labels.csv',\n",
       " 'CZI.PBMC.X.test.nn.notnorm.feather',\n",
       " 'CZI.PBMC.10.brcds',\n",
       " 'CZI.PBMC.2.brcds',\n",
       " 'CZI.PBMC.3.umis.mtx',\n",
       " 'logs',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.train.nn.pkl',\n",
       " 'push_cnn_lasso.py',\n",
       " 'CZI.PBMC.1.umis.mtx',\n",
       " 'CZI.PBMC.X.validate.nn.pkl',\n",
       " 'CZI.PBMC.7.umis.mtx',\n",
       " 'data_line_save_pbs3.sh',\n",
       " 'CZI.PBMC.5.umis.mtx',\n",
       " 'data_line_save4.py',\n",
       " 'CZI.PBMC.y.train.nn2.csv',\n",
       " 'data_line_save_notnorm.sh',\n",
       " 'doublet_nn_data_line_save.o10554779',\n",
       " 'first_try-Copy1.ipynb',\n",
       " 'first_try.py',\n",
       " 'CZI.PBMC.new.train.barcodes.pkl',\n",
       " 'CZI.PBMC.y.validate.nn.csv',\n",
       " 'doublet_nn_data_prep.o10545808',\n",
       " 'CZI.PBMC.X.test.nn2.csv',\n",
       " 'CZI.PBMC.10.umis.mtx',\n",
       " 'CZI.PBMC.X.train.nn2.csv',\n",
       " 'doublet_nn_data_line_save.o10554998',\n",
       " 'CZI.PBMC.X.validate.nn2.csv',\n",
       " 'CZI.PBMC.3.validate-Copy1.csv',\n",
       " 'CZI.PBMC.4.umis.mtx',\n",
       " 'CZI.PBMC.y.validate.nn.pkl',\n",
       " 'data_prep.py',\n",
       " 'CZI.PBMC.1.brcds',\n",
       " 'CZI.PBMC.3.train-Copy1.csv',\n",
       " 'dnn_autoencoder_notnorm.h5',\n",
       " 'data_line_save_pbs.sh',\n",
       " 'CZI.PBMC.X.test.nn.csv',\n",
       " 'CZI.PBMC.6.umis.mtx',\n",
       " 'CZI.PBMC.9.brcds',\n",
       " 'cnn_autoencoder_plain.h5',\n",
       " 'CZI.PBMC.y.test.nn2.csv',\n",
       " 'genes.nn.csv',\n",
       " 'CZI.PBMC.y.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596495',\n",
       " 'CZI.PBMC.gmm.sorted.labels.Rds',\n",
       " 'CZI.PBMC.X.train.nn.notnorm.feather',\n",
       " 'data_line_save_feather.e10596495',\n",
       " 'CZI.PBMC.full.X.nn.sorted.pkl',\n",
       " 'CZI.PBMC.full.X.nn.sorted.notnorm.feather',\n",
       " 'CZI.PBMC.new.validate.locs.pkl',\n",
       " 'CZI.PBMC.y.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.nn.pkl',\n",
       " 'data_line_save_feather.o10596482',\n",
       " 'CZI.PBMC.4.brcds',\n",
       " 'data_line_save.py',\n",
       " 'CZI.PBMC.X.train.nn.feather',\n",
       " 'CZI.PBMC.7.brcds',\n",
       " 'doublet_nn_data_prep.e10538694',\n",
       " 'CZI.PBMC.y.validate.nn2.csv',\n",
       " 'CZI.PBMC.new.train.locs.pkl',\n",
       " 'CZI.PBMC.X.train.nn2.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_locs=np.array([np.where(~all_brcds.Barcode.isin(val_tst_brcds.Barcode).values)])[0,0,:]\n",
    "#all_brcds.shape\n",
    "#not True\n",
    "#train_brcds=all_brcds.iloc[train_locs,]\n",
    "#pd.DataFrame(train_brcds.Barcode.values).to_pickle(\"CZI.PBMC.train.barcodes.pkl\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 11:42:57.984647 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0829 11:42:58.025861 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0829 11:42:58.277834 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0829 11:42:58.278930 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0829 11:42:58.296361 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0829 11:42:59.372035 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0829 11:43:04.561870 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0829 11:43:04.569002 46912496324416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#bebe=pd.read_pickle(\"CZI.PBMC.new.train.locs.pkl\")\n",
    "\n",
    "#pd.DataFrame(train_locs,columns=np.array([\"Locs\"])).to_pickle(\"CZI.PBMC.new.train.locs.pkl\")\n",
    "#pd.DataFrame(y.index.values[train_locs],columns=np.array([\"Barcode\"])).to_pickle(\"CZI.PBMC.new.train.barcodes.pkl\")\n",
    "\n",
    "#pd.DataFrame(validate_locs,columns=np.array([\"Locs\"])).to_pickle(\"CZI.PBMC.new.validate.locs.pkl\")\n",
    "#pd.DataFrame(y.index.values[validate_locs],columns=np.array([\"Barcode\"])).to_pickle(\"CZI.PBMC.new.validate.barcodes.pkl\")\n",
    "\n",
    "#pd.DataFrame(test_locs,columns=np.array([\"Locs\"])).to_pickle(\"CZI.PBMC.new.test.locs.pkl\")\n",
    "#pd.DataFrame(y.index.values[test_locs],columns=np.array([\"Barcode\"])).to_pickle(\"CZI.PBMC.new.test.barcodes.pkl\")\n",
    "cnn_autoencoder=load_model('cnn_autoencoder_plain_moreepoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorboard.plugins.pr_curve import summary as pr_summary\n",
    "\n",
    "# Check complete example in:\n",
    "# https://github.com/akionakamura/pr-tensorboard-keras-example\n",
    "class PRTensorBoard(TensorBoard):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # One extra argument to indicate whether or not to use the PR curve summary.\n",
    "        self.pr_curve = kwargs.pop('pr_curve', True)\n",
    "        super(PRTensorBoard, self).__init__(*args, **kwargs)\n",
    "\n",
    "        global tf\n",
    "        import tensorflow as tf\n",
    "\n",
    "    def set_model(self, model):\n",
    "        super(PRTensorBoard, self).set_model(model)\n",
    "\n",
    "        if self.pr_curve:\n",
    "            # Get the prediction and label tensor placeholders.\n",
    "            predictions = self.model._feed_outputs[0]\n",
    "            labels = tf.cast(self.model._feed_targets[0], tf.bool)\n",
    "            # Create the PR summary OP.\n",
    "            self.pr_summary = pr_summary.op(tag='pr_curve',\n",
    "                                            predictions=predictions,\n",
    "                                            labels=labels,\n",
    "                                            display_name='Precision-Recall Curve')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(PRTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "        if self.pr_curve and self.validation_data:\n",
    "            # Get the tensors again.\n",
    "            tensors = self.model._feed_targets + self.model._feed_outputs\n",
    "            # Predict the output.\n",
    "            predictions = self.model.predict(self.validation_data[:-2])\n",
    "            # Build the dictionary mapping the tensor to the data.\n",
    "            val_data = [self.validation_data[-2], predictions]\n",
    "            feed_dict = dict(zip(tensors, val_data))\n",
    "            # Run and add summary.\n",
    "            result = self.sess.run([self.pr_summary], feed_dict=feed_dict)\n",
    "            self.writer.add_summary(result[0], epoch)\n",
    "            self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_63 (Conv1D)           (None, 18488, 128)        384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 18488, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 18488, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 9244, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 9244, 64)          24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 4622, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 4622, 32)          6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 2311, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 2309, 1)           97        \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2309)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 750)               1732500   \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 750)               563250    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 751       \n",
      "=================================================================\n",
      "Total params: 2,328,310\n",
      "Trainable params: 2,296,598\n",
      "Non-trainable params: 31,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The known number of output classes.\n",
    "#num_classes = 2\n",
    "\n",
    "# Input image dimensions\n",
    "#input_shape = (18488,)\n",
    "\n",
    "##cnn_autoencoder=load_model('cnn_autoencoder_plain_moreepoch.h5')\n",
    "\n",
    "#cnn_autoencoder = Sequential()\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(128,kernel_size=3, input_shape=(N_feat,1), use_bias=False,padding='same',kernel_initializer=\"uniform\"))\n",
    "\n",
    "#cnn_autoencoder.add(BatchNormalization())\n",
    "#cnn_autoencoder.add(Activation('relu'))\n",
    "#cnn_autoencoder.add(ActivityRegularization(l1=10e-6,l2=10e-6))\n",
    "#cnn_autoencoder.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(64,kernel_size=3,kernel_initializer='uniform',activation='relu',padding='same'))\n",
    "#cnn_autoencoder.add(Dropout(0.2))\n",
    "#cnn_autoencoder.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(32,kernel_size=3,kernel_initializer='uniform',activation='relu',padding='same'))\n",
    "\n",
    "#cnn_autoencoder.add(UpSampling1D(2))\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(64,kernel_size=3,kernel_initializer='uniform',activation='relu',padding='same'))\n",
    "\n",
    "#cnn_autoencoder.add(UpSampling1D(2))\n",
    "#cnn_autoencoder.add(Conv1D(1,kernel_size=3,kernel_initializer='uniform',activation='sigmoid',padding=\"same\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cnn_autoencoder.compile(loss=keras.losses.binary_crossentropy, #mean_squared_error\n",
    " #             optimizer=keras.optimizers.Adam(),\n",
    " #             metrics=['accuracy'])\n",
    "\n",
    "###cnn_autoencoder.summary()\n",
    "\n",
    "encoder_layer0 = cnn_autoencoder.layers[0]\n",
    "encoder_layer0.trainable=False\n",
    "encoder_layer1 = cnn_autoencoder.layers[1]\n",
    "encoder_layer1.trainable=False\n",
    "encoder_layer2 = cnn_autoencoder.layers[2]\n",
    "encoder_layer2.trainable=False\n",
    "encoder_layer3 = cnn_autoencoder.layers[3]\n",
    "encoder_layer3.trainable=False\n",
    "encoder_layer4 = cnn_autoencoder.layers[4]\n",
    "encoder_layer4.trainable=False\n",
    "encoder_layer5 = cnn_autoencoder.layers[5]\n",
    "encoder_layer5.trainable=False\n",
    "encoder_layer6 = cnn_autoencoder.layers[6]\n",
    "encoder_layer6.trainable=False\n",
    "#dnn_encoder = Model(input_img, encoder_layer5(encoder_layer4(encoder_layer3(encoder_layer2(encoder_layer1(input_img))))))\n",
    "\n",
    "\n",
    "class_model=Sequential()\n",
    "class_model.add(encoder_layer0)\n",
    "class_model.add(encoder_layer1)\n",
    "class_model.add(encoder_layer2)\n",
    "class_model.add(encoder_layer3)\n",
    "class_model.add(encoder_layer4)\n",
    "class_model.add(encoder_layer5)\n",
    "class_model.add(encoder_layer6)\n",
    "class_model.add(MaxPooling1D(2))\n",
    "class_model.add(Conv1D(1,3))\n",
    "#class_model.add(MaxPooling1D(2))\n",
    "class_model.add(Flatten())\n",
    "class_model.add(Dense(750,activation='relu',activity_regularizer=regularizers.l1_l2(3.5e-6,3.5e-6)))\n",
    "class_model.add(Dropout(0.2))\n",
    "class_model.add(Dense(750,activation='relu'))\n",
    "\n",
    "#class_model.add(Dropout(0.2))\n",
    "#class_model.add(Dense(256,activation='relu'))#,activity_regularizer=regularizers.l1(20e-6)))\n",
    "#class_model.add(Dense(512,activation='relu'))#,activity_regularizer=regularizers.l1(20e-6)))\n",
    "\n",
    "#class_model.add(Dropout(0.2))\n",
    "#class_model.add(Dense(512,activation='relu'))\n",
    "#class_model.add(Dense(256,activation='relu'))\n",
    "\n",
    "class_model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "class_model.compile(loss=keras.losses.binary_crossentropy, #mean_squared_error\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy',keras_metrics.binary_precision(), keras_metrics.binary_recall(),keras_metrics.binary_f1_score()])#,\n",
    "                  # callbacks=[metricS])\n",
    "\n",
    "class_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#encoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.7198 - acc: 0.5583 - precision: 0.3236 - recall: 0.5810 - f1_score: 0.4157 - val_loss: 0.6230 - val_acc: 0.6730 - val_precision: 0.4022 - val_recall: 0.3978 - val_f1_score: 0.4000\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.5170 - acc: 0.6252 - precision: 0.3836 - recall: 0.6365 - f1_score: 0.4787 - val_loss: 0.6631 - val_acc: 0.5900 - val_precision: 0.3534 - val_recall: 0.5985 - val_f1_score: 0.4444\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.4707 - acc: 0.6819 - precision: 0.4425 - recall: 0.6783 - f1_score: 0.5356 - val_loss: 0.6659 - val_acc: 0.6170 - val_precision: 0.3613 - val_recall: 0.5182 - val_f1_score: 0.4258\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3658 - acc: 0.7933 - precision: 0.5894 - recall: 0.7766 - f1_score: 0.6702 - val_loss: 0.7126 - val_acc: 0.6770 - val_precision: 0.3948 - val_recall: 0.3358 - val_f1_score: 0.3629\n",
      "Epoch 5/50\n",
      " 5120/10000 [==============>...............] - ETA: 4s - loss: 0.2563 - acc: 0.8781 - precision: 0.7383 - recall: 0.8486 - f1_score: 0.7896"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-be18e56e8286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validate_minmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                        callbacks=callback)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mallDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "log_dir = os.path.join(current_dir, 'logs')\n",
    "\n",
    "if not os.path.exists(log_dir) or not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "N0=np.sum(y_validate==0)\n",
    "N1=np.sum(y_validate==1)\n",
    "w0 = (1 / N0) / (0.5 * (1 / N0 + 1 / N1))\n",
    "w1 = (1 / N1) / (0.5 * (1 / N0 + 1 / N1))\n",
    "class_weights={0:w0,1:w1}\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "#callback = [ PRTensorBoard(log_dir=log_dir), EarlyStopping(monitor='val_loss', patience=3) ]\n",
    "\n",
    "callback = [EarlyStopping(monitor='val_loss', patience=5)]#None#[EarlyStopping(monitor='loss', patience=5), EarlyStopping(monitor='val_loss', patience=5) ]\n",
    "\n",
    "\n",
    "history=class_model.fit(X_train_minmax[0:10000,:,:], y_train[0:10000,:],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_validate_minmax[0:1000,:,:], y_validate[0:1000,:]),class_weight=class_weights,\n",
    "                       callbacks=callback)\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1_train=pd.DataFrame(X_train_minmax)\n",
    "df1_validate=pd.DataFrame(X_validate_minmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_validate_minmax_pred=np.array(dnn_autoencoder_notnorm.predict(X_validate_minmax))\n",
    "X_validate_minmax_pred=np.array(cnn_autoencoder.predict(X_validate_minmax,batch_size=256))\n",
    "X_train_minmax_pred=np.array(cnn_autoencoder.predict(X_train_minmax,batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_validate_minmax[10,]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_minmax_pred=np.array(X_train_minmax_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_validate_minmax_pred[10,]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=scipy.io.mmread('czi_pbmc_notnorm.mtx').todense()\n",
    "X=scipy.io.mmread('CZI.PBMC.1.umis.mtx').todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(N_train_dat,N_feat,1)\n",
    "X_validate=X_validate.reshape(N_val_dat,N_feat,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ActivityRegularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_autoencoder.save('cnn_autoencoder_plain_moreepoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer0.layer_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate_encode_pred=np.array(class_model.predict(X_validate_minmax,batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder_layer0, encoder_layer1, encoder_layer2, encoder_layer3, encoder_layer4, encoder_layer5, encoder_layer6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_validate==0)/np.sum(y_validate==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=np.sum(y_validate==0)\n",
    "N1=np.sum(y_validate==1)\n",
    "w0 = (1 / N0) / (0.5 * (1 / N0 + 1 / N1))\n",
    "w1 = (1 / N1) / (0.5 * (1 / N0 + 1 / N1))\n",
    "class_weights={0:w0,1:w1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_model.layers[7].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242744, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras_metrics.metric_fn.<locals>.fn(label=0, **kwargs)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    keras_metrics.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "log_dir = os.path.join(current_dir, 'logs')\n",
    "\n",
    "if not os.path.exists(log_dir) or not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dnn_autoencoder_clr.h5',\n",
       " 'CZI.PBMC.X.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.csv',\n",
       " 'CZI.PBMC.new.validate.barcodes.pkl',\n",
       " 'cnn_autoencoder_plain_moreepoch.h5',\n",
       " 'iris.csv',\n",
       " 'CZI.PBMC.5.brcds',\n",
       " '.ipynb_checkpoints',\n",
       " 'CZI.PBMC.8.brcds',\n",
       " 'CZI.PBMC.test.barcodes.pkl',\n",
       " 'CZI.PBMC.2.umis.mtx',\n",
       " 'CZI.PBMC.3.brcds',\n",
       " 'CZI.PBMC.3.test-Copy1.csv',\n",
       " 'CZI.PBMC.X.validate.nn.csv',\n",
       " 'CZI.PBMC.9.umis.mtx',\n",
       " 'CZI.PBMC.X.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596475',\n",
       " 'data_line_save3.py',\n",
       " 'CZI.PBMC.full.X.nn.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.notnorm.feather',\n",
       " 'cnn_autoencoder_lasso.h5',\n",
       " 'CZI.PBMC.X.test.nn.feather',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.feather',\n",
       " 'CZI.PBMC.y.train.nn.feather',\n",
       " 'CZI.PBMC.train.barcodes.pkl',\n",
       " 'CZI.PBMC.new.test.barcodes.pkl',\n",
       " 'czi_pbmc_notnorm.mtx',\n",
       " 'data_prep_pbs.sh',\n",
       " 'first_try.ipynb',\n",
       " 'data_line_save_pbs2.sh',\n",
       " 'CZI.PBMC.X.test.nn.pkl',\n",
       " 'sonar_tutorial.ipynb',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.feather',\n",
       " 'data_line_save_feather.e10596475',\n",
       " 'CZI.PBMC.6.brcds',\n",
       " 'data_line_save_pbs4.sh',\n",
       " 'CZI.PBMC.non.neg.brcds.GMM.csv',\n",
       " 'doublet_nn_data_line_save.e10554779',\n",
       " 'data_line_save_feather.e10558828',\n",
       " 'CZI.PBMC.8.umis.mtx',\n",
       " 'data_line_save_feather.e10596482',\n",
       " 'data_line_save_feather.o10558828',\n",
       " 'doublet_nn_data_prep.o10538694',\n",
       " 'doublet_nn_data_line_save.e10551142',\n",
       " 'data_line_notnorm.py',\n",
       " 'doublet_nn_data_prep.e10545808',\n",
       " 'iris_tutorial.ipynb',\n",
       " 'cnn_transfer_classify.ipynb',\n",
       " 'CZI.PBMC.full.y.nn.sorted.cleaned.feather',\n",
       " 'CZI.PBMC.y.train.nn2.pkl',\n",
       " 'cnn_autoencoder.ipynb',\n",
       " 'dnn_autoencoder.ipynb',\n",
       " 'CZI.PBMC.validate.barcodes.pkl',\n",
       " 'dede.pkl',\n",
       " 'doublet_nn_data_line_save.e10554998',\n",
       " 'CZI.PBMC.new.test.locs.pkl',\n",
       " 'doublet_nn_data_line_save.o10551142',\n",
       " 'CZI.PBMC.y.validate.nn.feather',\n",
       " 'data_line_save2.py',\n",
       " 'sonar.csv',\n",
       " 'CZI.PBMC.y.test.nn.feather',\n",
       " 'CZI.PBMC.gmm.sorted.labels.csv',\n",
       " 'CZI.PBMC.X.test.nn.notnorm.feather',\n",
       " 'CZI.PBMC.10.brcds',\n",
       " 'CZI.PBMC.2.brcds',\n",
       " 'CZI.PBMC.3.umis.mtx',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.train.nn.pkl',\n",
       " 'push_cnn_lasso.py',\n",
       " 'CZI.PBMC.1.umis.mtx',\n",
       " 'CZI.PBMC.X.validate.nn.pkl',\n",
       " 'CZI.PBMC.7.umis.mtx',\n",
       " 'data_line_save_pbs3.sh',\n",
       " 'CZI.PBMC.5.umis.mtx',\n",
       " 'data_line_save4.py',\n",
       " 'CZI.PBMC.y.train.nn2.csv',\n",
       " 'data_line_save_notnorm.sh',\n",
       " 'doublet_nn_data_line_save.o10554779',\n",
       " 'first_try-Copy1.ipynb',\n",
       " 'first_try.py',\n",
       " 'CZI.PBMC.new.train.barcodes.pkl',\n",
       " 'CZI.PBMC.y.validate.nn.csv',\n",
       " 'doublet_nn_data_prep.o10545808',\n",
       " 'CZI.PBMC.X.test.nn2.csv',\n",
       " 'CZI.PBMC.10.umis.mtx',\n",
       " 'CZI.PBMC.X.train.nn2.csv',\n",
       " 'doublet_nn_data_line_save.o10554998',\n",
       " 'CZI.PBMC.X.validate.nn2.csv',\n",
       " 'CZI.PBMC.3.validate-Copy1.csv',\n",
       " 'CZI.PBMC.4.umis.mtx',\n",
       " 'CZI.PBMC.y.validate.nn.pkl',\n",
       " 'data_prep.py',\n",
       " 'CZI.PBMC.1.brcds',\n",
       " 'CZI.PBMC.3.train-Copy1.csv',\n",
       " 'dnn_autoencoder_notnorm.h5',\n",
       " 'data_line_save_pbs.sh',\n",
       " 'CZI.PBMC.X.test.nn.csv',\n",
       " 'CZI.PBMC.6.umis.mtx',\n",
       " 'CZI.PBMC.9.brcds',\n",
       " 'cnn_autoencoder_plain.h5',\n",
       " 'CZI.PBMC.y.test.nn2.csv',\n",
       " 'genes.nn.csv',\n",
       " 'CZI.PBMC.y.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596495',\n",
       " 'CZI.PBMC.gmm.sorted.labels.Rds',\n",
       " 'CZI.PBMC.X.train.nn.notnorm.feather',\n",
       " 'data_line_save_feather.e10596495',\n",
       " 'CZI.PBMC.full.X.nn.sorted.pkl',\n",
       " 'CZI.PBMC.full.X.nn.sorted.notnorm.feather',\n",
       " 'CZI.PBMC.new.validate.locs.pkl',\n",
       " 'CZI.PBMC.y.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.nn.pkl',\n",
       " 'data_line_save_feather.o10596482',\n",
       " 'CZI.PBMC.4.brcds',\n",
       " 'data_line_save.py',\n",
       " 'CZI.PBMC.X.train.nn.feather',\n",
       " 'CZI.PBMC.7.brcds',\n",
       " 'doublet_nn_data_prep.e10538694',\n",
       " 'CZI.PBMC.y.validate.nn2.csv',\n",
       " 'CZI.PBMC.new.train.locs.pkl',\n",
       " 'CZI.PBMC.X.train.nn2.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import up sound alert dependencies\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "## Insert whatever audio file you want above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
