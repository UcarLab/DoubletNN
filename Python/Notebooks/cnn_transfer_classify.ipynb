{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "#import keras as kr\n",
    "import sklearn as skl\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten, BatchNormalization, Conv1D, MaxPooling1D, Activation, UpSampling1D,Reshape\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import feather\n",
    "import keras_metrics\n",
    "## Import up sound alert dependencies\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "## Insert whatever audio file you want above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_validate=pd.read_pickle(\"CZI.PBMC.y.validate.nn.pkl\")\n",
    "#y_test=pd.read_pickle(\"CZI.PBMC.y.test.nn.pkl\")\n",
    "#X=pd.read_pickle(\"CZI.PBMC.full.X.nn.sorted.cleaned.pkl\")\n",
    "#y_train=pd.read_pickle(\"CZI.PBMC.y.tra\")\n",
    "y_train=pd.read_feather('CZI.PBMC.y.train.nn.feather').values\n",
    "X_train=pd.read_feather('CZI.PBMC.X.train.nn.feather').values\n",
    "#X_train=pd.read_feather('CZI.PBMC.X.train.nn.notnorm.feather')\n",
    "\n",
    "\n",
    "y_validate=pd.read_feather('CZI.PBMC.y.validate.nn.feather').values\n",
    "X_validate=pd.read_feather('CZI.PBMC.X.validate.nn.feather').values\n",
    "#X_validate=pd.read_feather('CZI.PBMC.X.validate.nn.notnorm.feather')\n",
    "\n",
    "\n",
    "N_train_dat=X_train.shape[0]\n",
    "#N_test_dat=\n",
    "N_val_dat=X_validate.shape[0]\n",
    "N_feat=X_validate.shape[1]\n",
    "\n",
    "#X_train=X_train.values.reshape(N_train_dat,N_feat,1)\n",
    "#X_validate=X_validate.values.reshape(N_val_dat,N_feat,1)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_validate_minmax = min_max_scaler.fit_transform(X_validate)\n",
    "\n",
    "X_train_minmax=X_train_minmax.reshape(N_train_dat,N_feat,1)\n",
    "X_validate_minmax=X_validate_minmax.reshape(N_val_dat,N_feat,1)\n",
    "\n",
    "\n",
    "#input_dir = os.getcwd()\n",
    "#genes=pd.read_csv(\"genes.nn.csv\").x[1:]\n",
    "\n",
    "#counts_matrix={}\n",
    "#for i in range(1,11,2):\n",
    "#    brcds_1=pd.read_csv(input_dir+'/CZI.PBMC.'+str(i)+'.brcds').x\n",
    "#    counts_matrix_1=scipy.io.mmread(input_dir + '/CZI.PBMC.'+str(i)+'.umis.mtx').T.tocsc()\n",
    "#    brcds_2=pd.read_csv(input_dir+'/CZI.PBMC.'+str(i+1)+'.brcds').x\n",
    "#    counts_matrix_2=scipy.io.mmread(input_dir + '/CZI.PBMC.'+str(i+1)+'.umis.mtx').T.tocsc()\n",
    "#    df_1=pd.DataFrame(counts_matrix_1.todense(),index=brcds_1,columns=genes)\n",
    " #   df_2=pd.DataFrame(counts_matrix_2.todense(),index=brcds_2,columns=genes)\n",
    "#    if i==1:\n",
    "#        czi_pbmc=pd.concat([df_1,df_2])\n",
    "#    else:\n",
    "#        czi_pbmc=pd.concat([czi_pbmc,df_1,df_2])\n",
    "#        \n",
    "#        \n",
    "#czi_pbmc.to_pickle(\"CZI.PBMC.full.nn.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_x_train = pd.read_csv(\"/projects/ucar-lab/danaco/bncmrk-dblts/DF/input/CZI.PBMC/CZI.PBMC.2.train.csv\", header=None)\n",
    "#df_x_validate = pd.read_csv(\"/projects/ucar-lab/danaco/bncmrk-dblts/DF/input/CZI.PBMC/CZI.PBMC.2.validate.csv\", header=None)\n",
    "#df_x_test = pd.read_csv(\"/projects/ucar-lab/danaco/bncmrk-dblts/DF/input/CZI.PBMC/CZI.PBMC.2.test.csv\", header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_targ = self.model.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\" — vali_f1: %f — vali_precision: %f — vali_recall %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    " \n",
    "\n",
    "metricS = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dnn_autoencoder_clr.h5',\n",
       " 'CZI.PBMC.X.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.csv',\n",
       " 'CZI.PBMC.new.validate.barcodes.pkl',\n",
       " 'cnn_autoencoder_plain_moreepoch.h5',\n",
       " 'iris.csv',\n",
       " 'CZI.PBMC.5.brcds',\n",
       " '.ipynb_checkpoints',\n",
       " 'CZI.PBMC.8.brcds',\n",
       " 'CZI.PBMC.test.barcodes.pkl',\n",
       " 'CZI.PBMC.2.umis.mtx',\n",
       " 'CZI.PBMC.3.brcds',\n",
       " 'CZI.PBMC.3.test-Copy1.csv',\n",
       " 'CZI.PBMC.X.validate.nn.csv',\n",
       " 'CZI.PBMC.9.umis.mtx',\n",
       " 'CZI.PBMC.X.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596475',\n",
       " 'data_line_save3.py',\n",
       " 'CZI.PBMC.full.X.nn.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.notnorm.feather',\n",
       " 'cnn_autoencoder_lasso.h5',\n",
       " 'CZI.PBMC.X.test.nn.feather',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.feather',\n",
       " 'CZI.PBMC.y.train.nn.feather',\n",
       " 'CZI.PBMC.train.barcodes.pkl',\n",
       " 'CZI.PBMC.new.test.barcodes.pkl',\n",
       " 'czi_pbmc_notnorm.mtx',\n",
       " 'data_prep_pbs.sh',\n",
       " 'first_try.ipynb',\n",
       " 'data_line_save_pbs2.sh',\n",
       " 'CZI.PBMC.X.test.nn.pkl',\n",
       " 'sonar_tutorial.ipynb',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.feather',\n",
       " 'data_line_save_feather.e10596475',\n",
       " 'CZI.PBMC.6.brcds',\n",
       " 'data_line_save_pbs4.sh',\n",
       " 'CZI.PBMC.non.neg.brcds.GMM.csv',\n",
       " 'doublet_nn_data_line_save.e10554779',\n",
       " 'data_line_save_feather.e10558828',\n",
       " 'CZI.PBMC.8.umis.mtx',\n",
       " 'data_line_save_feather.e10596482',\n",
       " 'data_line_save_feather.o10558828',\n",
       " 'doublet_nn_data_prep.o10538694',\n",
       " 'doublet_nn_data_line_save.e10551142',\n",
       " 'data_line_notnorm.py',\n",
       " 'doublet_nn_data_prep.e10545808',\n",
       " 'iris_tutorial.ipynb',\n",
       " 'cnn_transfer_classify.ipynb',\n",
       " 'CZI.PBMC.full.y.nn.sorted.cleaned.feather',\n",
       " 'CZI.PBMC.y.train.nn2.pkl',\n",
       " 'cnn_autoencoder.ipynb',\n",
       " 'dnn_autoencoder.ipynb',\n",
       " 'CZI.PBMC.validate.barcodes.pkl',\n",
       " 'dede.pkl',\n",
       " 'doublet_nn_data_line_save.e10554998',\n",
       " 'CZI.PBMC.new.test.locs.pkl',\n",
       " 'doublet_nn_data_line_save.o10551142',\n",
       " 'CZI.PBMC.y.validate.nn.feather',\n",
       " 'data_line_save2.py',\n",
       " 'sonar.csv',\n",
       " 'CZI.PBMC.y.test.nn.feather',\n",
       " 'CZI.PBMC.gmm.sorted.labels.csv',\n",
       " 'CZI.PBMC.X.test.nn.notnorm.feather',\n",
       " 'CZI.PBMC.10.brcds',\n",
       " 'CZI.PBMC.2.brcds',\n",
       " 'CZI.PBMC.3.umis.mtx',\n",
       " 'logs',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.train.nn.pkl',\n",
       " 'push_cnn_lasso.py',\n",
       " 'CZI.PBMC.1.umis.mtx',\n",
       " 'CZI.PBMC.X.validate.nn.pkl',\n",
       " 'CZI.PBMC.7.umis.mtx',\n",
       " 'data_line_save_pbs3.sh',\n",
       " 'CZI.PBMC.5.umis.mtx',\n",
       " 'data_line_save4.py',\n",
       " 'CZI.PBMC.y.train.nn2.csv',\n",
       " 'data_line_save_notnorm.sh',\n",
       " 'doublet_nn_data_line_save.o10554779',\n",
       " 'first_try-Copy1.ipynb',\n",
       " 'first_try.py',\n",
       " 'CZI.PBMC.new.train.barcodes.pkl',\n",
       " 'CZI.PBMC.y.validate.nn.csv',\n",
       " 'doublet_nn_data_prep.o10545808',\n",
       " 'CZI.PBMC.X.test.nn2.csv',\n",
       " 'CZI.PBMC.10.umis.mtx',\n",
       " 'CZI.PBMC.X.train.nn2.csv',\n",
       " 'doublet_nn_data_line_save.o10554998',\n",
       " 'CZI.PBMC.X.validate.nn2.csv',\n",
       " 'CZI.PBMC.3.validate-Copy1.csv',\n",
       " 'CZI.PBMC.4.umis.mtx',\n",
       " 'CZI.PBMC.y.validate.nn.pkl',\n",
       " 'data_prep.py',\n",
       " 'CZI.PBMC.1.brcds',\n",
       " 'CZI.PBMC.3.train-Copy1.csv',\n",
       " 'dnn_autoencoder_notnorm.h5',\n",
       " 'data_line_save_pbs.sh',\n",
       " 'CZI.PBMC.X.test.nn.csv',\n",
       " 'CZI.PBMC.6.umis.mtx',\n",
       " 'CZI.PBMC.9.brcds',\n",
       " 'cnn_autoencoder_plain.h5',\n",
       " 'CZI.PBMC.y.test.nn2.csv',\n",
       " 'genes.nn.csv',\n",
       " 'CZI.PBMC.y.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596495',\n",
       " 'CZI.PBMC.gmm.sorted.labels.Rds',\n",
       " 'CZI.PBMC.X.train.nn.notnorm.feather',\n",
       " 'data_line_save_feather.e10596495',\n",
       " 'CZI.PBMC.full.X.nn.sorted.pkl',\n",
       " 'CZI.PBMC.full.X.nn.sorted.notnorm.feather',\n",
       " 'CZI.PBMC.new.validate.locs.pkl',\n",
       " 'CZI.PBMC.y.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.nn.pkl',\n",
       " 'data_line_save_feather.o10596482',\n",
       " 'CZI.PBMC.4.brcds',\n",
       " 'data_line_save.py',\n",
       " 'CZI.PBMC.X.train.nn.feather',\n",
       " 'CZI.PBMC.7.brcds',\n",
       " 'doublet_nn_data_prep.e10538694',\n",
       " 'CZI.PBMC.y.validate.nn2.csv',\n",
       " 'CZI.PBMC.new.train.locs.pkl',\n",
       " 'CZI.PBMC.X.train.nn2.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_locs=np.array([np.where(~all_brcds.Barcode.isin(val_tst_brcds.Barcode).values)])[0,0,:]\n",
    "#all_brcds.shape\n",
    "#not True\n",
    "#train_brcds=all_brcds.iloc[train_locs,]\n",
    "#pd.DataFrame(train_brcds.Barcode.values).to_pickle(\"CZI.PBMC.train.barcodes.pkl\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 06:51:20.117908 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0829 06:51:20.159262 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0829 06:51:20.403329 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0829 06:51:20.404419 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0829 06:51:20.421202 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0829 06:51:21.533251 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0829 06:51:27.064166 46912496324416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0829 06:51:27.072604 46912496324416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#bebe=pd.read_pickle(\"CZI.PBMC.new.train.locs.pkl\")\n",
    "\n",
    "#pd.DataFrame(train_locs,columns=np.array([\"Locs\"])).to_pickle(\"CZI.PBMC.new.train.locs.pkl\")\n",
    "#pd.DataFrame(y.index.values[train_locs],columns=np.array([\"Barcode\"])).to_pickle(\"CZI.PBMC.new.train.barcodes.pkl\")\n",
    "\n",
    "#pd.DataFrame(validate_locs,columns=np.array([\"Locs\"])).to_pickle(\"CZI.PBMC.new.validate.locs.pkl\")\n",
    "#pd.DataFrame(y.index.values[validate_locs],columns=np.array([\"Barcode\"])).to_pickle(\"CZI.PBMC.new.validate.barcodes.pkl\")\n",
    "\n",
    "#pd.DataFrame(test_locs,columns=np.array([\"Locs\"])).to_pickle(\"CZI.PBMC.new.test.locs.pkl\")\n",
    "#pd.DataFrame(y.index.values[test_locs],columns=np.array([\"Barcode\"])).to_pickle(\"CZI.PBMC.new.test.barcodes.pkl\")\n",
    "cnn_autoencoder=load_model('cnn_autoencoder_plain_moreepoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorboard.plugins.pr_curve import summary as pr_summary\n",
    "\n",
    "# Check complete example in:\n",
    "# https://github.com/akionakamura/pr-tensorboard-keras-example\n",
    "class PRTensorBoard(TensorBoard):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # One extra argument to indicate whether or not to use the PR curve summary.\n",
    "        self.pr_curve = kwargs.pop('pr_curve', True)\n",
    "        super(PRTensorBoard, self).__init__(*args, **kwargs)\n",
    "\n",
    "        global tf\n",
    "        import tensorflow as tf\n",
    "\n",
    "    def set_model(self, model):\n",
    "        super(PRTensorBoard, self).set_model(model)\n",
    "\n",
    "        if self.pr_curve:\n",
    "            # Get the prediction and label tensor placeholders.\n",
    "            predictions = self.model._feed_outputs[0]\n",
    "            labels = tf.cast(self.model._feed_targets[0], tf.bool)\n",
    "            # Create the PR summary OP.\n",
    "            self.pr_summary = pr_summary.op(tag='pr_curve',\n",
    "                                            predictions=predictions,\n",
    "                                            labels=labels,\n",
    "                                            display_name='Precision-Recall Curve')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super(PRTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "        if self.pr_curve and self.validation_data:\n",
    "            # Get the tensors again.\n",
    "            tensors = self.model._feed_targets + self.model._feed_outputs\n",
    "            # Predict the output.\n",
    "            predictions = self.model.predict(self.validation_data[:-2])\n",
    "            # Build the dictionary mapping the tensor to the data.\n",
    "            val_data = [self.validation_data[-2], predictions]\n",
    "            feed_dict = dict(zip(tensors, val_data))\n",
    "            # Run and add summary.\n",
    "            result = self.sess.run([self.pr_summary], feed_dict=feed_dict)\n",
    "            self.writer.add_summary(result[0], epoch)\n",
    "            self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_63 (Conv1D)           (None, 18488, 128)        384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 18488, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 18488, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 9244, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 9244, 64)          24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 4622, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 4622, 32)          6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2311, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2309, 1)           97        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2309)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1182720   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,477,698\n",
      "Trainable params: 1,445,986\n",
      "Non-trainable params: 31,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The known number of output classes.\n",
    "#num_classes = 2\n",
    "\n",
    "# Input image dimensions\n",
    "#input_shape = (18488,)\n",
    "\n",
    "##cnn_autoencoder=load_model('cnn_autoencoder_plain_moreepoch.h5')\n",
    "\n",
    "#cnn_autoencoder = Sequential()\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(128,kernel_size=3, input_shape=(N_feat,1), use_bias=False,padding='same',kernel_initializer=\"uniform\"))\n",
    "\n",
    "#cnn_autoencoder.add(BatchNormalization())\n",
    "#cnn_autoencoder.add(Activation('relu'))\n",
    "#cnn_autoencoder.add(ActivityRegularization(l1=10e-6,l2=10e-6))\n",
    "#cnn_autoencoder.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(64,kernel_size=3,kernel_initializer='uniform',activation='relu',padding='same'))\n",
    "#cnn_autoencoder.add(Dropout(0.2))\n",
    "#cnn_autoencoder.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(32,kernel_size=3,kernel_initializer='uniform',activation='relu',padding='same'))\n",
    "\n",
    "#cnn_autoencoder.add(UpSampling1D(2))\n",
    "\n",
    "#cnn_autoencoder.add(Conv1D(64,kernel_size=3,kernel_initializer='uniform',activation='relu',padding='same'))\n",
    "\n",
    "#cnn_autoencoder.add(UpSampling1D(2))\n",
    "#cnn_autoencoder.add(Conv1D(1,kernel_size=3,kernel_initializer='uniform',activation='sigmoid',padding=\"same\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cnn_autoencoder.compile(loss=keras.losses.binary_crossentropy, #mean_squared_error\n",
    " #             optimizer=keras.optimizers.Adam(),\n",
    " #             metrics=['accuracy'])\n",
    "\n",
    "###cnn_autoencoder.summary()\n",
    "\n",
    "encoder_layer0 = cnn_autoencoder.layers[0]\n",
    "encoder_layer0.trainable=False\n",
    "encoder_layer1 = cnn_autoencoder.layers[1]\n",
    "encoder_layer1.trainable=False\n",
    "encoder_layer2 = cnn_autoencoder.layers[2]\n",
    "encoder_layer2.trainable=False\n",
    "encoder_layer3 = cnn_autoencoder.layers[3]\n",
    "encoder_layer3.trainable=False\n",
    "encoder_layer4 = cnn_autoencoder.layers[4]\n",
    "encoder_layer4.trainable=False\n",
    "encoder_layer5 = cnn_autoencoder.layers[5]\n",
    "encoder_layer5.trainable=False\n",
    "encoder_layer6 = cnn_autoencoder.layers[6]\n",
    "encoder_layer6.trainable=False\n",
    "#dnn_encoder = Model(input_img, encoder_layer5(encoder_layer4(encoder_layer3(encoder_layer2(encoder_layer1(input_img))))))\n",
    "\n",
    "\n",
    "class_model=Sequential()\n",
    "class_model.add(encoder_layer0)\n",
    "class_model.add(encoder_layer1)\n",
    "class_model.add(encoder_layer2)\n",
    "class_model.add(encoder_layer3)\n",
    "class_model.add(encoder_layer4)\n",
    "class_model.add(encoder_layer5)\n",
    "class_model.add(encoder_layer6)\n",
    "class_model.add(MaxPooling1D(2))\n",
    "class_model.add(Conv1D(1,3))\n",
    "#class_model.add(MaxPooling1D(2))\n",
    "class_model.add(Flatten())\n",
    "#class_model.add(Dense(1024,activation='relu'))\n",
    "#class_model.add(Dropout(0.2))\n",
    "class_model.add(Dense(512,activation='relu'))#,activity_regularizer=regularizers.l1(20e-6)))\n",
    "#class_model.add(Dropout(0.2))\n",
    "class_model.add(Dense(512,activation='relu'))\n",
    "class_model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "class_model.compile(loss=keras.losses.binary_crossentropy, #mean_squared_error\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy',keras_metrics.binary_precision(), keras_metrics.binary_recall(),keras_metrics.binary_f1_score()])#,\n",
    "                  # callbacks=[metricS])\n",
    "\n",
    "class_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#encoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242744 samples, validate on 30343 samples\n",
      "Epoch 1/50\n",
      "242744/242744 [==============================] - 518s 2ms/step - loss: 0.5306 - acc: 0.6628 - precision: 0.4135 - recall: 0.5533 - f1_score: 0.4733 - val_loss: 0.5979 - val_acc: 0.7106 - val_precision: 0.4765 - val_recall: 0.4785 - val_f1_score: 0.4775\n",
      "Epoch 2/50\n",
      "242744/242744 [==============================] - 515s 2ms/step - loss: 0.4995 - acc: 0.6916 - precision: 0.4471 - recall: 0.5355 - f1_score: 0.4874 - val_loss: 0.6326 - val_acc: 0.6786 - val_precision: 0.4359 - val_recall: 0.5536 - val_f1_score: 0.4877\n",
      "Epoch 3/50\n",
      "242744/242744 [==============================] - 521s 2ms/step - loss: 0.4923 - acc: 0.6981 - precision: 0.4569 - recall: 0.5444 - f1_score: 0.4969 - val_loss: 0.6363 - val_acc: 0.6590 - val_precision: 0.4175 - val_recall: 0.5921 - val_f1_score: 0.4897\n",
      "Epoch 4/50\n",
      "242744/242744 [==============================] - 517s 2ms/step - loss: 0.4848 - acc: 0.7013 - precision: 0.4629 - recall: 0.5680 - f1_score: 0.5101 - val_loss: 0.6184 - val_acc: 0.6845 - val_precision: 0.4422 - val_recall: 0.5423 - val_f1_score: 0.4871\n",
      "Epoch 5/50\n",
      "242744/242744 [==============================] - 514s 2ms/step - loss: 0.4724 - acc: 0.7072 - precision: 0.4728 - recall: 0.6043 - f1_score: 0.5305 - val_loss: 0.6159 - val_acc: 0.6875 - val_precision: 0.4438 - val_recall: 0.5166 - val_f1_score: 0.4774\n",
      "Epoch 6/50\n",
      "242744/242744 [==============================] - 519s 2ms/step - loss: 0.4445 - acc: 0.7206 - precision: 0.4925 - recall: 0.6712 - f1_score: 0.5681 - val_loss: 0.6810 - val_acc: 0.6258 - val_precision: 0.3868 - val_recall: 0.6050 - val_f1_score: 0.4719\n",
      "Epoch 7/50\n",
      "242744/242744 [==============================] - 516s 2ms/step - loss: 0.3862 - acc: 0.7547 - precision: 0.5367 - recall: 0.7611 - f1_score: 0.6295 - val_loss: 0.6893 - val_acc: 0.6408 - val_precision: 0.3897 - val_recall: 0.5301 - val_f1_score: 0.4492\n",
      "Epoch 8/50\n",
      "242744/242744 [==============================] - 514s 2ms/step - loss: 0.3082 - acc: 0.8049 - precision: 0.6036 - recall: 0.8370 - f1_score: 0.7014 - val_loss: 0.8349 - val_acc: 0.6520 - val_precision: 0.3984 - val_recall: 0.5079 - val_f1_score: 0.4465\n",
      "Epoch 9/50\n",
      "242744/242744 [==============================] - 511s 2ms/step - loss: 0.2434 - acc: 0.8461 - precision: 0.6651 - recall: 0.8822 - f1_score: 0.7584 - val_loss: 1.0362 - val_acc: 0.6485 - val_precision: 0.3918 - val_recall: 0.4924 - val_f1_score: 0.4364\n",
      "Epoch 10/50\n",
      "153088/242744 [=================>............] - ETA: 3:21 - loss: 0.1908 - acc: 0.8795 - precision: 0.7203 - recall: 0.9151 - f1_score: 0.8061"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "log_dir = os.path.join(current_dir, 'logs')\n",
    "\n",
    "if not os.path.exists(log_dir) or not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "N0=np.sum(y_validate==0)\n",
    "N1=np.sum(y_validate==1)\n",
    "w0 = (1 / N0) / (0.5 * (1 / N0 + 1 / N1))\n",
    "w1 = (1 / N1) / (0.5 * (1 / N0 + 1 / N1))\n",
    "class_weights={0:w0,1:w1}\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "#callback = [ PRTensorBoard(log_dir=log_dir), EarlyStopping(monitor='val_loss', patience=3) ]\n",
    "\n",
    "callback = None#[ EarlyStopping(monitor='val_loss', patience=4) ]\n",
    "\n",
    "\n",
    "history=class_model.fit(X_train_minmax, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_validate_minmax, y_validate),class_weight=class_weights,\n",
    "                       callbacks=callback)\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1_train=pd.DataFrame(X_train_minmax)\n",
    "df1_validate=pd.DataFrame(X_validate_minmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_validate_minmax_pred=np.array(dnn_autoencoder_notnorm.predict(X_validate_minmax))\n",
    "X_validate_minmax_pred=np.array(cnn_autoencoder.predict(X_validate_minmax,batch_size=256))\n",
    "X_train_minmax_pred=np.array(cnn_autoencoder.predict(X_train_minmax,batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_validate_minmax[10,]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_minmax_pred=np.array(X_train_minmax_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_validate_minmax_pred[10,]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=scipy.io.mmread('czi_pbmc_notnorm.mtx').todense()\n",
    "X=scipy.io.mmread('CZI.PBMC.1.umis.mtx').todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(N_train_dat,N_feat,1)\n",
    "X_validate=X_validate.reshape(N_val_dat,N_feat,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ActivityRegularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_autoencoder.save('cnn_autoencoder_plain_moreepoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer0.layer_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate_encode_pred=np.array(class_model.predict(X_validate_minmax,batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder_layer0, encoder_layer1, encoder_layer2, encoder_layer3, encoder_layer4, encoder_layer5, encoder_layer6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_validate==0)/np.sum(y_validate==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=np.sum(y_validate==0)\n",
    "N1=np.sum(y_validate==1)\n",
    "w0 = (1 / N0) / (0.5 * (1 / N0 + 1 / N1))\n",
    "w1 = (1 / N1) / (0.5 * (1 / N0 + 1 / N1))\n",
    "class_weights={0:w0,1:w1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_model.layers[7].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242744, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras_metrics.metric_fn.<locals>.fn(label=0, **kwargs)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    keras_metrics.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "log_dir = os.path.join(current_dir, 'logs')\n",
    "\n",
    "if not os.path.exists(log_dir) or not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dnn_autoencoder_clr.h5',\n",
       " 'CZI.PBMC.X.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.csv',\n",
       " 'CZI.PBMC.new.validate.barcodes.pkl',\n",
       " 'cnn_autoencoder_plain_moreepoch.h5',\n",
       " 'iris.csv',\n",
       " 'CZI.PBMC.5.brcds',\n",
       " '.ipynb_checkpoints',\n",
       " 'CZI.PBMC.8.brcds',\n",
       " 'CZI.PBMC.test.barcodes.pkl',\n",
       " 'CZI.PBMC.2.umis.mtx',\n",
       " 'CZI.PBMC.3.brcds',\n",
       " 'CZI.PBMC.3.test-Copy1.csv',\n",
       " 'CZI.PBMC.X.validate.nn.csv',\n",
       " 'CZI.PBMC.9.umis.mtx',\n",
       " 'CZI.PBMC.X.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596475',\n",
       " 'data_line_save3.py',\n",
       " 'CZI.PBMC.full.X.nn.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.notnorm.feather',\n",
       " 'cnn_autoencoder_lasso.h5',\n",
       " 'CZI.PBMC.X.test.nn.feather',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.validate.nn.feather',\n",
       " 'CZI.PBMC.y.train.nn.feather',\n",
       " 'CZI.PBMC.train.barcodes.pkl',\n",
       " 'CZI.PBMC.new.test.barcodes.pkl',\n",
       " 'czi_pbmc_notnorm.mtx',\n",
       " 'data_prep_pbs.sh',\n",
       " 'first_try.ipynb',\n",
       " 'data_line_save_pbs2.sh',\n",
       " 'CZI.PBMC.X.test.nn.pkl',\n",
       " 'sonar_tutorial.ipynb',\n",
       " 'CZI.PBMC.full.X.nn.sorted.cleaned.feather',\n",
       " 'data_line_save_feather.e10596475',\n",
       " 'CZI.PBMC.6.brcds',\n",
       " 'data_line_save_pbs4.sh',\n",
       " 'CZI.PBMC.non.neg.brcds.GMM.csv',\n",
       " 'doublet_nn_data_line_save.e10554779',\n",
       " 'data_line_save_feather.e10558828',\n",
       " 'CZI.PBMC.8.umis.mtx',\n",
       " 'data_line_save_feather.e10596482',\n",
       " 'data_line_save_feather.o10558828',\n",
       " 'doublet_nn_data_prep.o10538694',\n",
       " 'doublet_nn_data_line_save.e10551142',\n",
       " 'data_line_notnorm.py',\n",
       " 'doublet_nn_data_prep.e10545808',\n",
       " 'iris_tutorial.ipynb',\n",
       " 'cnn_transfer_classify.ipynb',\n",
       " 'CZI.PBMC.full.y.nn.sorted.cleaned.feather',\n",
       " 'CZI.PBMC.y.train.nn2.pkl',\n",
       " 'cnn_autoencoder.ipynb',\n",
       " 'dnn_autoencoder.ipynb',\n",
       " 'CZI.PBMC.validate.barcodes.pkl',\n",
       " 'dede.pkl',\n",
       " 'doublet_nn_data_line_save.e10554998',\n",
       " 'CZI.PBMC.new.test.locs.pkl',\n",
       " 'doublet_nn_data_line_save.o10551142',\n",
       " 'CZI.PBMC.y.validate.nn.feather',\n",
       " 'data_line_save2.py',\n",
       " 'sonar.csv',\n",
       " 'CZI.PBMC.y.test.nn.feather',\n",
       " 'CZI.PBMC.gmm.sorted.labels.csv',\n",
       " 'CZI.PBMC.X.test.nn.notnorm.feather',\n",
       " 'CZI.PBMC.10.brcds',\n",
       " 'CZI.PBMC.2.brcds',\n",
       " 'CZI.PBMC.3.umis.mtx',\n",
       " 'CZI.PBMC.full.Y.nn.sorted.cleaned.pkl',\n",
       " 'CZI.PBMC.X.train.nn.pkl',\n",
       " 'push_cnn_lasso.py',\n",
       " 'CZI.PBMC.1.umis.mtx',\n",
       " 'CZI.PBMC.X.validate.nn.pkl',\n",
       " 'CZI.PBMC.7.umis.mtx',\n",
       " 'data_line_save_pbs3.sh',\n",
       " 'CZI.PBMC.5.umis.mtx',\n",
       " 'data_line_save4.py',\n",
       " 'CZI.PBMC.y.train.nn2.csv',\n",
       " 'data_line_save_notnorm.sh',\n",
       " 'doublet_nn_data_line_save.o10554779',\n",
       " 'first_try-Copy1.ipynb',\n",
       " 'first_try.py',\n",
       " 'CZI.PBMC.new.train.barcodes.pkl',\n",
       " 'CZI.PBMC.y.validate.nn.csv',\n",
       " 'doublet_nn_data_prep.o10545808',\n",
       " 'CZI.PBMC.X.test.nn2.csv',\n",
       " 'CZI.PBMC.10.umis.mtx',\n",
       " 'CZI.PBMC.X.train.nn2.csv',\n",
       " 'doublet_nn_data_line_save.o10554998',\n",
       " 'CZI.PBMC.X.validate.nn2.csv',\n",
       " 'CZI.PBMC.3.validate-Copy1.csv',\n",
       " 'CZI.PBMC.4.umis.mtx',\n",
       " 'CZI.PBMC.y.validate.nn.pkl',\n",
       " 'data_prep.py',\n",
       " 'CZI.PBMC.1.brcds',\n",
       " 'CZI.PBMC.3.train-Copy1.csv',\n",
       " 'dnn_autoencoder_notnorm.h5',\n",
       " 'data_line_save_pbs.sh',\n",
       " 'CZI.PBMC.X.test.nn.csv',\n",
       " 'CZI.PBMC.6.umis.mtx',\n",
       " 'CZI.PBMC.9.brcds',\n",
       " 'cnn_autoencoder_plain.h5',\n",
       " 'CZI.PBMC.y.test.nn2.csv',\n",
       " 'genes.nn.csv',\n",
       " 'CZI.PBMC.y.test.nn2.pkl',\n",
       " 'data_line_save_feather.o10596495',\n",
       " 'CZI.PBMC.gmm.sorted.labels.Rds',\n",
       " 'CZI.PBMC.X.train.nn.notnorm.feather',\n",
       " 'data_line_save_feather.e10596495',\n",
       " 'CZI.PBMC.full.X.nn.sorted.pkl',\n",
       " 'CZI.PBMC.full.X.nn.sorted.notnorm.feather',\n",
       " 'CZI.PBMC.new.validate.locs.pkl',\n",
       " 'CZI.PBMC.y.validate.nn2.pkl',\n",
       " 'CZI.PBMC.full.nn.pkl',\n",
       " 'data_line_save_feather.o10596482',\n",
       " 'CZI.PBMC.4.brcds',\n",
       " 'data_line_save.py',\n",
       " 'CZI.PBMC.X.train.nn.feather',\n",
       " 'CZI.PBMC.7.brcds',\n",
       " 'doublet_nn_data_prep.e10538694',\n",
       " 'CZI.PBMC.y.validate.nn2.csv',\n",
       " 'CZI.PBMC.new.train.locs.pkl',\n",
       " 'CZI.PBMC.X.train.nn2.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import up sound alert dependencies\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "## Insert whatever audio file you want above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
